# server/app.py (ìµœì¢… ì™„ì„±ë³¸)
from fastapi import FastAPI, File, UploadFile, Form
from PIL import Image
import torch
import torch.nn as nn
from transformers import CLIPProcessor, CLIPModel
import io

app = FastAPI()
device = "cpu"

# ==========================================
# 1. ë‘ ê°€ì§€ ëª¨ë¸ ì„¤ê³„ë„ ëª¨ë‘ ì¤€ë¹„
# ==========================================

# (A) ê°€ë²¼ìš´ ëª¨ë¸ (LightStyleNet)
class LightStyleNet(nn.Module):
    def __init__(self, num_classes=4):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(0.3),
            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.2),
            nn.Linear(128, num_classes)
        )
    def forward(self, x): return self.layers(x)

# (B) ê¹Šì€ ëª¨ë¸ (StyleNet)
class StyleNet(nn.Module):
    def __init__(self, num_classes=4):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(512, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(0.5),
            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )
    def forward(self, x): return self.layers(x)

# ==========================================
# 2. ìŠ¤ë§ˆíŠ¸ ë¡œë”© í•¨ìˆ˜
# ==========================================
def smart_load(path, model_name):
    print(f"ğŸ”„ {model_name} ë¡œë”© ì‹œë„ ì¤‘...")
    
    # ì‹œë„ 1: StyleNet (ê¹Šì€ ê±°)
    try:
        model = StyleNet(num_classes=4)
        model.load_state_dict(torch.load(path, map_location=device))
        model.eval()
        print(f"âœ… {model_name}: StyleNet êµ¬ì¡°ë¡œ ë¡œë”© ì„±ê³µ!")
        return model
    except:
        pass 

    # ì‹œë„ 2: LightStyleNet (ê°€ë²¼ìš´ ê±°)
    try:
        model = LightStyleNet(num_classes=4)
        model.load_state_dict(torch.load(path, map_location=device))
        model.eval()
        print(f"âœ… {model_name}: LightStyleNet êµ¬ì¡°ë¡œ ë¡œë”© ì„±ê³µ!")
        return model
    except:
        print(f"âŒ {model_name}: ë¡œë”© ì‹¤íŒ¨! (íŒŒì¼ì´ ê¹¨ì¡Œê±°ë‚˜ êµ¬ì¡°ê°€ ì•„ì˜ˆ ë‹¤ë¦„)")
        return None

# ==========================================
# 3. ëª¨ë¸ ì¤€ë¹„
# ==========================================
print("â³ ëª¨ë¸ ì¤€ë¹„ ì¤‘...")
try:
    clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
    clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
    print("âœ… CLIP ë¡œë“œ ì™„ë£Œ")
except:
    print("âŒ CLIP ë¡œë“œ ì‹¤íŒ¨")

models = {}
models['male'] = smart_load("male_model.pth", "ë‚¨ì ëª¨ë¸")
models['female'] = smart_load("female_model.pth", "ì—¬ì ëª¨ë¸")


# ==========================================
# 4. ì˜ˆì¸¡ API
# ==========================================
@app.post("/predict")
async def predict(gender: str = Form(...), file: UploadFile = File(...)):
    # ì´ë¯¸ì§€ ì½ê¸°
    image_bytes = await file.read()
    image = Image.open(io.BytesIO(image_bytes)).convert('RGB')

    # CLIP íŠ¹ì§• ì¶”ì¶œ
    inputs = clip_processor(images=image, return_tensors="pt", padding=True)
    with torch.no_grad():
        features = clip_model.get_image_features(**inputs)
        features /= features.norm(dim=-1, keepdim=True)

    # ëª¨ë¸ ì„ íƒ
    target_model = models.get(gender)
    if target_model is None:
        return {"result": f"Error: {gender} ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨"}

    # ì˜ˆì¸¡
    with torch.no_grad():
        outputs = target_model(features)
        _, predicted = torch.max(outputs, 1)
        
    idx = predicted.item()

    # â˜… ê²°ê³¼ ë¼ë²¨ë§ (ì—¬ê¸°ê°€ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤!)
    if gender == 'male':
        # ë‚¨ì
        style_names = ['Americaji Vintage', 'Casual', 'Minimal Chic Dandy', 'Street Gorpcore']
        result = style_names[idx]
        
    else:
        # ì—¬ì: ì•Œë ¤ì£¼ì‹  ìˆœì„œ ê·¸ëŒ€ë¡œ ì ìš©!
        female_styles = ['Casual Street', 'Feminine Minimal', 'Lovely', 'Unique']
        
        if idx < len(female_styles):
            result = female_styles[idx]
        else:
            result = "Unknown"

    return {"result": result}