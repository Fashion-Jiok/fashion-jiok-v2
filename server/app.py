# server/app.py

from fastapi import FastAPI, File, UploadFile, Form
from PIL import Image
import torch
import torch.nn as nn
import torch.nn.functional as F  # í™•ë¥  ê³„ì‚°ìš© (softmax)
from transformers import CLIPProcessor, CLIPModel
import io
import uvicorn  # ì„œë²„ ì‹¤í–‰ìš©

app = FastAPI()
device = "cpu"  # GPUê°€ ìˆë‹¤ë©´ "cuda"

# ==========================================
# 1. ëª¨ë¸ ì•„í‚¤í…ì²˜ ì •ì˜ (ê¸°ì¡´ê³¼ ë™ì¼)
# ==========================================

# (A) ê°€ë²¼ìš´ ëª¨ë¸ (LightStyleNet)
class LightStyleNet(nn.Module):
    def __init__(self, num_classes=4):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(0.3),
            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.2),
            nn.Linear(128, num_classes)
        )
    def forward(self, x): return self.layers(x)

# (B) ê¹Šì€ ëª¨ë¸ (StyleNet)
class StyleNet(nn.Module):
    def __init__(self, num_classes=4):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(512, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(0.5),
            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )
    def forward(self, x): return self.layers(x)

# ==========================================
# 2. ìŠ¤ë§ˆíŠ¸ ë¡œë”© í•¨ìˆ˜
# ==========================================
def smart_load(path, model_name):
    print(f"ğŸ”„ {model_name} ë¡œë”© ì‹œë„ ì¤‘...")
    
    # ì‹œë„ 1: StyleNet
    try:
        model = StyleNet(num_classes=4)
        model.load_state_dict(torch.load(path, map_location=device))
        model.eval()
        print(f"âœ… {model_name}: StyleNet êµ¬ì¡°ë¡œ ë¡œë”© ì„±ê³µ!")
        return model
    except:
        pass 

    # ì‹œë„ 2: LightStyleNet
    try:
        model = LightStyleNet(num_classes=4)
        model.load_state_dict(torch.load(path, map_location=device))
        model.eval()
        print(f"âœ… {model_name}: LightStyleNet êµ¬ì¡°ë¡œ ë¡œë”© ì„±ê³µ!")
        return model
    except:
        print(f"âŒ {model_name}: ë¡œë”© ì‹¤íŒ¨! (íŒŒì¼ ê²½ë¡œ í™•ì¸ í•„ìš”)")
        return None

# ==========================================
# 3. ëª¨ë¸ ì¤€ë¹„
# ==========================================
print("â³ ëª¨ë¸ ì¤€ë¹„ ì¤‘...")
try:
    clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
    clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
    print("âœ… CLIP ë¡œë“œ ì™„ë£Œ")
except Exception as e:
    print(f"âŒ CLIP ë¡œë“œ ì‹¤íŒ¨: {e}")

models = {}
# ê°™ì€ í´ë”ì— .pth íŒŒì¼ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
models['male'] = smart_load("male_model.pth", "ë‚¨ì ëª¨ë¸")
models['female'] = smart_load("female_model.pth", "ì—¬ì ëª¨ë¸")


# ==========================================
# 4. ì˜ˆì¸¡ API (ì•±ìœ¼ë¡œ í™•ë¥  ì •ë³´ ì „ì†¡)
# ==========================================
@app.post("/predict")
async def predict(gender: str = Form(...), file: UploadFile = File(...)):
    # 1. ì´ë¯¸ì§€ ì½ê¸°
    image_bytes = await file.read()
    image = Image.open(io.BytesIO(image_bytes)).convert('RGB')
    
    # â­ï¸ [ì†ë„ ìµœì í™”] ì´ë¯¸ì§€ í¬ê¸° ê°•ì œ ì¶•ì†Œ
    image = image.resize((224, 224))

    # 2. CLIP íŠ¹ì§• ì¶”ì¶œ
    inputs = clip_processor(images=image, return_tensors="pt", padding=True)
    with torch.no_grad():
        features = clip_model.get_image_features(**inputs)
        features /= features.norm(dim=-1, keepdim=True)

    # 3. ëª¨ë¸ ì„ íƒ
    target_model = models.get(gender)
    if target_model is None:
        return {"result": f"Error: {gender} ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨"}

    # 4. ì˜ˆì¸¡ ë° í™•ë¥  ê³„ì‚°
    with torch.no_grad():
        outputs = target_model(features)
        
        # í™•ë¥ (%) ê³„ì‚°
        probs = F.softmax(outputs, dim=1) 
        top_prob, predicted = torch.max(probs, 1)
        
    idx = predicted.item()
    confidence_val = top_prob.item() * 100 # ìˆ«ìê°’ (ì˜ˆ: 85.5)

    # 5. ê²°ê³¼ ë¼ë²¨ë§
    if gender == 'male':
        # ë‚¨ì ë¼ë²¨ ìˆœì„œ (í•™ìŠµëœ ìˆœì„œì™€ ê°™ì•„ì•¼ í•¨)
        style_names = ['Americaji Vintage', 'Casual', 'Minimal Chic Dandy', 'Street Gorpcore']
        result = style_names[idx] if idx < len(style_names) else "Unknown"
        
    else:
        # ì—¬ì ë¼ë²¨ ìˆœì„œ
        female_styles = ['Casual Street', 'Feminine Minimal', 'Lovely', 'Unique']
        result = female_styles[idx] if idx < len(female_styles) else "Unknown"

    # ==================================================
    # â­ï¸ [í•µì‹¬] ì•±ìœ¼ë¡œ ë³´ë‚¼ ë°ì´í„° (í™•ë¥  í¬í•¨)
    # ==================================================
    
    # í™•ë¥  ë¦¬ìŠ¤íŠ¸ ë³´ê¸° ì¢‹ê²Œ ë³€í™˜ (ì˜ˆ: [10.5, 80.2, 5.0, 4.3])
    prob_list = [round(p * 100, 1) for p in probs.tolist()[0]]

    # ì„œë²„ í„°ë¯¸ë„ì—ë„ ë¡œê·¸ ì¶œë ¥
    print(f"ğŸ“¸ ìš”ì²­: {gender} | ê²°ê³¼: {result} ({confidence_val:.1f}%)")
    print(f"ğŸ“Š ë¶„í¬: {prob_list}")

    return {
        "result": result,                 # ìŠ¤íƒ€ì¼ ì´ë¦„
        "confidence": f"{confidence_val:.1f}%", # í™•ì‹ ë„ (ë¬¸ìì—´)
        "probabilities": prob_list        # ì „ì²´ í™•ë¥  ë¶„í¬ (ë°°ì—´)
    }

if __name__ == "__main__":
    print("ğŸš€ AI ë¶„ì„ ì„œë²„ ì‹œì‘ (í¬íŠ¸: 8000)")
    uvicorn.run(app, host="0.0.0.0", port=8000)